<!DOCTYPE html>
        <html class="inverted" lang="en">
        <head>
        <meta http-equiv="content-type" content="text/html; charset=UTF-8">
        <meta charset="utf-8">
        <meta name="theme-color" content="#ffffff">
        <title>F# Neural Network - joshj.dev</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="Josh's website and portfolio.">
        <link rel="stylesheet" type="text/css" href="../files/main.css">
        <!-- Icons-->
        <link rel="apple-touch-icon" sizes="180x180" href="../files/icons/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="../files/icons/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="../files/icons/favicon-16x16.png">
        </head>
        <body>
        <!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title></title>
  
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #303030; color: #cccccc; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; }
td.sourceCode { padding-left: 5px; }
pre { color: #cccccc; background-color: #303030; }
code > span.kw { color: #f0dfaf; } /* Keyword */
code > span.dt { color: #dfdfbf; } /* DataType */
code > span.dv { color: #dcdccc; } /* DecVal */
code > span.bn { color: #dca3a3; } /* BaseN */
code > span.fl { color: #c0bed1; } /* Float */
code > span.ch { color: #dca3a3; } /* Char */
code > span.st { color: #cc9393; } /* String */
code > span.co { color: #7f9f7f; } /* Comment */
code > span.ot { color: #efef8f; } /* Other */
code > span.al { color: #ffcfaf; } /* Alert */
code > span.fu { color: #efef8f; } /* Function */
code > span.er { color: #c3bf9f; } /* Error */
code > span.wa { color: #7f9f7f; font-weight: bold; } /* Warning */
code > span.cn { color: #dca3a3; font-weight: bold; } /* Constant */
code > span.sc { color: #dca3a3; } /* SpecialChar */
code > span.vs { color: #cc9393; } /* VerbatimString */
code > span.ss { color: #cc9393; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { } /* Variable */
code > span.cf { color: #f0dfaf; } /* ControlFlow */
code > span.op { color: #f0efd0; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #ffcfaf; font-weight: bold; } /* Preprocessor */
code > span.at { } /* Attribute */
code > span.do { color: #7f9f7f; } /* Documentation */
code > span.an { color: #7f9f7f; font-weight: bold; } /* Annotation */
code > span.cv { color: #7f9f7f; font-weight: bold; } /* CommentVar */
code > span.in { color: #7f9f7f; font-weight: bold; } /* Information */
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="f-neural-network">F# Neural Network</h1>
<p><div class="fancyPositioning"></p>
<p><intro date="25/04/2019"></p>
<div class="picture-left">
<figure>
<img src="../files/images/neuralnetwork.png" alt="A neural network attempts to immitate a brain." /><figcaption>A neural network attempts to immitate a brain.</figcaption>
</figure></div>
<div class="tleft">
<p>A small F# library that allows for the creation of scalable fully-connected neural networks. It was developed for and built entirely using F#.</p>
<p></intro></p>
<ul>
<li><a href="#usage">Usage</a></li>
<li><a href="#how-it-works">How it works</a></li>
<li><a href="#github-repository">More Info</a></li>
</ul>
<p></div>
</div></p>
<h2 id="usage">Usage</h2>
<h3 id="defining-a-network">Defining a network</h3>
<p>Networks can be specified using the following format. Any number of layers are supported. Provided they align, the dimensions can be as large as desired.</p>
<div class="sourceCode"><pre class="sourceCode fsharp"><code class="sourceCode fsharp"><span class="kw">let</span> network = [
    {inputDims = <span class="dv">3</span>; outputDims = <span class="dv">5</span>; activation = Sigmoid};
    {inputDims = <span class="dv">5</span>; outputDims = <span class="dv">6</span>; activation = Sigmoid};
    {inputDims = <span class="dv">6</span>; outputDims = <span class="dv">2</span>; activation = Sigmoid};
]</code></pre></div>
<p>You can choose from multiple different activation functions including:</p>
<ul>
<li>Relu</li>
<li>Sigmoid</li>
<li>Tanh</li>
<li>Softmax</li>
<li>Leaky Relu</li>
<li>Elu</li>
<li>Selu</li>
<li>Softsign</li>
<li>Softplus</li>
<li>Exponential</li>
<li>Hard Sigmoid</li>
<li>Linear</li>
</ul>
<h3 id="training-a-network">Training a network</h3>
<p>Input data is provided in the form of a list of inputs, and a list of labels corresponding to each input.</p>
<div class="sourceCode"><pre class="sourceCode fsharp"><code class="sourceCode fsharp"><span class="co">(* Inputs *)</span>
<span class="kw">let</span> data = [
    [<span class="fl">0.5</span>; <span class="fl">1.0</span>; <span class="fl">0.2</span>];
    [<span class="fl">0.1</span>; <span class="fl">0.7</span>; <span class="fl">1.0</span>];
    [<span class="fl">1.0</span>; <span class="fl">0.1</span>; <span class="fl">0.1</span>];
    [<span class="fl">0.0</span>; <span class="fl">0.34</span>; <span class="fl">0.8</span>];
    [<span class="fl">0.6</span>; <span class="fl">0.1</span>; <span class="fl">0.3</span>]
]

<span class="co">(* Labels *)</span>
<span class="kw">let</span> labels = [
    [<span class="fl">1.0</span>; <span class="fl">1.0</span>];
    [<span class="fl">0.0</span>; <span class="fl">1.0</span>];
    [<span class="fl">0.0</span>; <span class="fl">0.0</span>];
    [<span class="fl">1.0</span>; <span class="fl">0.0</span>];
    [<span class="fl">0.0</span>; <span class="fl">1.0</span>];
]</code></pre></div>
<p>To train and run the network, see the code snippets below:</p>
<div class="sourceCode"><pre class="sourceCode fsharp"><code class="sourceCode fsharp"><span class="co">(* trainNetwork architecture labels data learning-rate loss iterations *)</span>
<span class="kw">let</span> model = trainNetwork network labels data <span class="fl">0.05</span> MSE <span class="dv">100000</span></code></pre></div>
<p>Currently, the following loss functions are avaliable:</p>
<ul>
<li>Mean Square Error</li>
<li>Cross Entropy</li>
<li>Mean Absolute Error</li>
</ul>
<h3 id="running-a-trained-network">Running a trained network</h3>
<p>A single run of the network can be specified as follows:</p>
<div class="sourceCode"><pre class="sourceCode fsharp"><code class="sourceCode fsharp"><span class="co">(* runNetwork model input architecture *)</span>
runNetwork model [<span class="fl">0.1</span>; <span class="fl">0.8</span>; <span class="fl">0.4</span>] network <span class="co">(* [0.90643753; 0.99834754] *)</span></code></pre></div>
<p>We can test multiple inputs by using a loop:</p>
<div class="sourceCode"><pre class="sourceCode fsharp"><code class="sourceCode fsharp"><span class="kw">for</span> idx <span class="kw">in</span> List<span class="kw">.</span>init (List<span class="kw">.</span>length data) id <span class="kw">do</span>
    printfn <span class="st">&quot;Input: %A&quot;</span> data.[idx]
    printfn <span class="st">&quot;Output: %A&quot;</span> (runNetwork model data.[idx] network)</code></pre></div>
<p>This will print the following:</p>
<div class="sourceCode"><pre class="sourceCode fsharp"><code class="sourceCode fsharp">Input: [<span class="fl">0.5</span>; <span class="fl">1.0</span>; <span class="fl">0.2</span>]
Output: [<span class="fl">0.9748251802</span>; <span class="fl">0.9991071572</span>]

Input: [<span class="fl">0.1</span>; <span class="fl">0.7</span>; <span class="fl">1.0</span>]
Output: [<span class="fl">0.04458155893</span>; <span class="fl">0.9556900964</span>]

Input: [<span class="fl">1.0</span>; <span class="fl">0.1</span>; <span class="fl">0.1</span>]
Output: [<span class="fl">1.150921027e-05</span>; <span class="fl">0.03353754142</span>]

Input: [<span class="fl">0.0</span>; <span class="fl">0.34</span>; <span class="fl">0.8</span>]
Output: [<span class="fl">0.9681691113</span>; <span class="fl">0.03517992806</span>]

Input: [<span class="fl">0.6</span>; <span class="fl">0.1</span>; <span class="fl">0.3</span>]
Output: [<span class="fl">0.003145187104</span>; <span class="fl">0.9791424116</span>] </code></pre></div>
<p>These match up with the labels specified earlier.</p>
<h2 id="how-it-works">How it works</h2>
<h3 id="forward-propagation">Forward Propagation</h3>
<p>A single forward propagation though a layer just involves multiplying the inputs to the layer with the layer weights (their dot product). After this, they are summed and the bias added before having the activation layer for that function applied.</p>
<div class="sourceCode"><pre class="sourceCode fsharp"><code class="sourceCode fsharp"><span class="kw">let</span> forwardSingleLayer (bias : <span class="dt">float</span>) (weights : <span class="dt">float</span> list list) (inputs : <span class="dt">float</span> list) (activation : Activation) : <span class="dt">float</span> list =
    weights
    |&gt; List<span class="kw">.</span>map (<span class="kw">fun</span> list -&gt; List<span class="kw">.</span>map2 ( * ) list inputs)
    |&gt; List<span class="kw">.</span>map (<span class="kw">fun</span> list -&gt; List<span class="kw">.</span>sum list + bias)
    |&gt; activateLayer activation </code></pre></div>
<p>The <code>activateLayer</code> function takes an activation and maps the corresponding activation function across the list before returning it. A small part of this function can be see below:</p>
<div class="sourceCode"><pre class="sourceCode fsharp"><code class="sourceCode fsharp"><span class="kw">let</span> activateLayer (activation : Activation) (input : <span class="dt">float</span> list) : <span class="dt">float</span> list =
    <span class="kw">match</span> activation <span class="kw">with</span>
    | Sigmoid -&gt; 
        List<span class="kw">.</span>map (<span class="kw">fun</span> x -&gt; <span class="fl">1.0</span> / (<span class="fl">1.0</span> + exp(-x))) input
    | Relu -&gt; 
        List<span class="kw">.</span>map (<span class="kw">fun</span> x -&gt; max x <span class="fl">0.0</span>) input</code></pre></div>
<p>The full forward propagation is slightly more complex. It first creates an empty list the size of the network. It folds though the epty list using the accumulator to store the intermediate activated and unactivated output of all layers. It then adds an extra layer containing only 1.0s to the end of the forward propagated output. This makes it easier when doing back propagation.</p>
<div class="sourceCode"><pre class="sourceCode fsharp"><code class="sourceCode fsharp"><span class="kw">let</span> forwardFull (parameters : Parameters) (inputs : <span class="dt">float</span> list) (layers : Layer list) : <span class="dt">float</span> list list =
    List<span class="kw">.</span>init layers.Length id
    |&gt; List<span class="kw">.</span>fold (<span class="kw">fun</span> (acc : <span class="dt">float</span> list list) index -&gt;
        <span class="kw">let</span> biases, weights, activation =
            parameters.biases.[index], parameters.weights.[index], layers.[index].activation
        [forwardSingleLayer biases weights acc.[<span class="dv">0</span>] activation] @ acc) 
            [inputs]
    |&gt; List<span class="kw">.</span>append [List<span class="kw">.</span>init (List<span class="kw">.</span>last layers).outputDims <span class="dt">float</span>]</code></pre></div>
<h3 id="calculating-error">Calculating Error</h3>
<p>To calculate the error in the output, the output of forward propagation and the target output are passed to a function that calculated the loss.</p>
<div class="sourceCode"><pre class="sourceCode fsharp"><code class="sourceCode fsharp"><span class="kw">let</span> getOverallError (targetOutputs : <span class="dt">float</span> list) (actualOutputs : <span class="dt">float</span> list) (loss : Loss) : <span class="dt">float</span> =
    actualOutputs
    |&gt; List<span class="kw">.</span>map2 (lossFunction loss targetOutputs.Length) targetOutputs
    |&gt; List<span class="kw">.</span>sum</code></pre></div>
<p>Similarly to the activation functions, the loss functions are supplied using pattern matching In this case however, they return a function that can be applied wherever the particular value required instead of taking an returning a list. Part of <code>lossFunction</code> can be seen below:</p>
<div class="sourceCode"><pre class="sourceCode fsharp"><code class="sourceCode fsharp"><span class="kw">let</span> lossFunction (loss : Loss) (n : <span class="dt">int</span>) : <span class="dt">float</span> -&gt; <span class="dt">float</span> -&gt; <span class="dt">float</span> =
    <span class="kw">match</span> loss <span class="kw">with</span>
    | MSE -&gt; 
        <span class="kw">fun</span> actual target -&gt; 
            (<span class="fl">1.0</span> / (n |&gt; <span class="dt">float</span>)) * (actual - target) ** <span class="fl">2.0</span>
    | MAE -&gt; 
        <span class="kw">fun</span> actual target -&gt; 
            (<span class="fl">1.0</span> / (n |&gt; <span class="dt">float</span>)) * (abs (actual - target))</code></pre></div>
<h3 id="back-propagation">Back Propagation</h3>
<p>Back propagation is more complex than the forward version. We first calculate the intermidate output sums. For the output layer, the only thing that needs to be done is for the derivatve of the loss function to applied to the outputs. For the hidden layers, the individual deltas for each node in the previous layer are found. Afterwards, the layer weights are taken and multiplied by their corresponding deltas. The values coming into each node are then summed.</p>
<p>The new weights are then calculated. This is simpler. The derivative of the activation function is found and then multiplied by the corresponding output delta sum. This is then multipliued by the output od of the previous layer resulting in a value corresponding to <code>learningRate * delta * outPrev</code>. This value is then subtracted from the corresponding weight as back propagation is defined.</p>
<p>The new weights and intermediate output deltas are then returned.</p>
<div class="sourceCode"><pre class="sourceCode fsharp"><code class="sourceCode fsharp"><span class="kw">let</span> backPropSingleLayer (targetOutputs : <span class="dt">float</span> list) (loss : Loss) (learningRate : <span class="dt">float</span>) (backPropPart : <span class="dt">float</span> list) (layerIndex : <span class="dt">int</span>) (forwardPropParts : <span class="dt">float</span> list list) (allWeights : <span class="dt">float</span> list list list) (layers : Layer list) : <span class="dt">float</span> list list * <span class="dt">float</span> list =
    
    <span class="kw">let</span> intermediateOutputDeltaSum =
        <span class="kw">if</span> layerIndex = <span class="dv">0</span>
        <span class="kw">then</span> <span class="co">(* Output layer *)</span>
            forwardPropParts.[layerIndex + <span class="dv">1</span>]
            |&gt; List<span class="kw">.</span>map2 (dLossFunction loss targetOutputs.Length) backPropPart 
        <span class="kw">else</span> <span class="co">(* Hidden layers *)</span>
            List<span class="kw">.</span>init forwardPropParts.[layerIndex + <span class="dv">1</span>].Length (<span class="kw">fun</span> _ -&gt;
                forwardPropParts.[layerIndex] 
                |&gt; dActivateLayer layers.[layerIndex - <span class="dv">1</span>].activation 
                |&gt; List<span class="kw">.</span>map2 ( * ) backPropPart) 
            |&gt; List<span class="kw">.</span>mapi (<span class="kw">fun</span> index1 deltas -&gt;
                allWeights.[layerIndex]
                |&gt; List<span class="kw">.</span>mapi (<span class="kw">fun</span> index2 weights -&gt; weights.[index1] * deltas.[index2])) 
            |&gt; List<span class="kw">.</span>map List<span class="kw">.</span>sum
    
    <span class="kw">let</span> newWeights =
        forwardPropParts.[layerIndex + <span class="dv">1</span>]
        |&gt; dActivateLayer layers.[layerIndex].activation
        |&gt; List<span class="kw">.</span>map2 ( * ) intermediateOutputDeltaSum
        |&gt; List<span class="kw">.</span>map (<span class="kw">fun</span> delta -&gt; 
            forwardPropParts.[layerIndex + <span class="dv">2</span>] 
            |&gt; List<span class="kw">.</span>map (<span class="kw">fun</span> out -&gt; learningRate * delta * out))
        |&gt; List<span class="kw">.</span>map2 (List<span class="kw">.</span>map2 (-)) allWeights.[layerIndex + <span class="dv">1</span>]
    
    newWeights, intermediateOutputDeltaSum</code></pre></div>
<p>The full back propagation invloved first reversing the parameters. This makes it easier to work on as we start from the final layer and work towards the input.</p>
<p>The next step is to fold through the network from the outer layer and calculate all the new values for each weight using <code>backPropSingleLayer</code>. This is then added to a list containing each layer and acts as the accumulator.</p>
<div class="sourceCode"><pre class="sourceCode fsharp"><code class="sourceCode fsharp"><span class="kw">let</span> backPropFull (layers : Layer list) (parameters : Parameters) (targetOutputs : <span class="dt">float</span> list list) (learningRate : <span class="dt">float</span>) (loss : Loss) (forwardPropParts : <span class="dt">float</span> list list) : (<span class="dt">float</span> list list * <span class="dt">float</span> list) list =  
    
    <span class="kw">let</span> weights = 
        List<span class="kw">.</span>rev parameters.weights

    List<span class="kw">.</span>init (layers.Length) id
    |&gt; List<span class="kw">.</span>fold (<span class="kw">fun</span> acc index -&gt;
        <span class="kw">let</span> targetOutputs =
            acc.[<span class="dv">0</span>] 
            |&gt; fst 
            |&gt; List<span class="kw">.</span>map List<span class="kw">.</span>sum
        
        <span class="kw">let</span> backPropPart = snd acc.[<span class="dv">0</span>]

        <span class="kw">let</span> backPropOutput = 
            backPropSingleLayer targetOutputs loss learningRate backPropPart index forwardPropParts weights layers
        
        [backPropOutput] @ acc) 
            [targetOutputs, (List<span class="kw">.</span>concat targetOutputs)]</code></pre></div>
<p>The derivatives of the activations and loss functions are very similar to the normal counter parts. Parts of the <code>dActivateLayer</code> and <code>dLossFunction</code> can be seen below:</p>
<div class="sourceCode"><pre class="sourceCode fsharp"><code class="sourceCode fsharp"><span class="kw">let</span> dActivateLayer (activation : Activation) (input : <span class="dt">float</span> list) : <span class="dt">float</span> list =
    <span class="kw">match</span> activation <span class="kw">with</span>
    | Sigmoid -&gt; 
        List<span class="kw">.</span>map (<span class="kw">fun</span> x -&gt; x * (<span class="fl">1.0</span> - x)) input
    | Relu -&gt; 
        List<span class="kw">.</span>map (<span class="kw">fun</span> x -&gt; <span class="kw">if</span> x &gt; <span class="fl">0.0</span> <span class="kw">then</span> <span class="fl">1.0</span> <span class="kw">else</span> <span class="fl">0.0</span>) input

<span class="kw">let</span> dLossFunction (loss : Loss) (n : <span class="dt">int</span>) : <span class="dt">float</span> -&gt; <span class="dt">float</span> -&gt; <span class="dt">float</span> =
    <span class="kw">match</span> loss <span class="kw">with</span>
    | MSE -&gt; 
    <span class="kw">fun</span> actual target -&gt; 
    (<span class="fl">2.0</span> / (n |&gt; <span class="dt">float</span>)) * (actual - target) * <span class="fl">-1.0</span>
    | MAE -&gt; 
    <span class="kw">fun</span> target actual -&gt; 
        <span class="kw">if</span> actual &gt; target <span class="kw">then</span> <span class="fl">1.0</span> <span class="kw">else</span> <span class="fl">-1.0</span></code></pre></div>
<h3 id="training">Training</h3>
<p>Training the network is a fairly simple recursive function that continually picks a random sample of the data set and carries out forwards and backwards propagation using the sample for any number of iterations. A simplified version of the function (initialisation and printing etc. removed) can be seen below:</p>
<div class="sourceCode"><pre class="sourceCode fsharp"><code class="sourceCode fsharp"><span class="kw">let</span> <span class="kw">rec</span> train (parameters : Parameters) (model : Layer list) (maxIterations : <span class="dt">int</span>) (iterations : <span class="dt">int</span>) =
    <span class="kw">let</span> idx = System<span class="kw">.</span>Random().Next(<span class="dv">0</span>, List<span class="kw">.</span>length targetOutputs)

    <span class="kw">let</span> fullSingle = 
        forwardFull parameters inputs.[idx] architecture
        |&gt; backPropFull architecture parameters targets.[idx] learningRate loss
        |&gt; weightUpdate parameters
    
    train fullSingle model maxIterations (iterations + <span class="dv">1</span>)
    
<span class="co">(* This is called like below *)</span>
train initial architecture iterations <span class="dv">0</span></code></pre></div>
<h2 id="bugs">Bugs</h2>
<p>Be careful with the chosen parameters. The networks can die easily if the chosen parameters cause weigths to overflow and become NaN, alternatively the network won't learn the data set correctly.</p>
<h2 id="github-repository">GitHub Repository</h2>
<p>The full project can be viewed on <a href="https://github.com/joshjennings98/fsharp-neural-network">GitHub.</a></p>
</body>
</html>

        <a id=title href="/"><div id="test">Go to homepage</div></a>
        <script src="../files/invert.js" type="text/javascript"></script>
        <footer><a href=#top>Back to top</a></footer>
        </body>  
        </html>